@article{eggly,
author = {Lyu, Yue and An, Pengcheng and Xiao, Yage and Zhang, Zibo and Zhang, Huan and Katsuragawa, Keiko and Zhao, Jian},
title = {Eggly: Designing Mobile Augmented Reality Neurofeedback Training Games for Children with Autism Spectrum Disorder},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3596251},
doi = {10.1145/3596251},
abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder that affects how children communicate and relate to other people and the world around them. Emerging studies have shown that neurofeedback training (NFT) games are an effective and playful intervention to enhance social and attentional capabilities for autistic children. However, NFT is primarily available in a clinical setting that is hard to scale. Also, the intervention demands deliberately-designed gamified feedback with fun and enjoyment, where little knowledge has been acquired in the HCI community. Through a ten-month iterative design process with four domain experts, we developed Eggly, a mobile NFT game based on a consumer-grade EEG headband and a tablet. Eggly uses novel augmented reality (AR) techniques to offer engagement and personalization, enhancing their training experience. We conducted two field studies (a single-session study and a three-week multi-session study) with a total of five autistic children to assess Eggly in practice at a special education center. Both quantitative and qualitative results indicate the effectiveness of the approach as well as contribute to the design knowledge of creating mobile AR NFT games.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {67},
numpages = {29},
keywords = {Autism spectrum disorder, EEG headband, augmented reality, mobile game, neurofeedback training}
}

@inproceedings{emowear,
author = {An, Pengcheng and Zhu, Jiawen Stefanie and Zhang, Zibo and Yin, Yifei and Ma, Qingyuan and Yan, Che and Du, Linghao and Zhao, Jian},
title = {EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642101},
doi = {10.1145/3613904.3642101},
abstract = {Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored “Emotional Teasers”—pre-retrieval cues offering a glimpse into an awaiting message’s emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders’ choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are distilled for future design. We thereby contribute both a novel system and empirical knowledge concerning emotional teasers for voice messaging.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {279},
numpages = {16},
keywords = {Animation, Emotion, Emotional Teasers, Smartwatch, Voice Message},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{crosscultural,
  title={Assessing the Prevalence of Cross-cultural Competencies in Engineering Design Curricula: A Pilot Study},
  author={Zhang, Zibo and Hurst, Ada},
  journal={Proceedings of the Canadian Engineering Education Association (CEEA)},
  year={2023}
}